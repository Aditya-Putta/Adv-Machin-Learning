---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---


# Processing the labels of the raw IMDB data

```{r}
library(keras)
library(tensorflow)

imdb_dir <- "aclImdb"
train_dir <- file.path(imdb_dir, "train")
labels <- c()
texts <- c()
for (label_type in c("neg", "pos")) {
  label <- switch(label_type, neg = 0, pos = 1)
  dir_name <- file.path(train_dir, label_type)
  for (fname in list.files(dir_name, pattern = glob2rx("*.txt"),
                           full.names = TRUE)) {
    texts <- c(texts, readChar(fname, file.info(fname)$size))
    labels <- c(labels, label)
  }
}
```

# Tokenizing the text of the raw IMDB data
```{r}
# considering length a 150 words

maxlen <- 150

# trainig data sample of 100
training_samples <- 100

# validation data of 10000 
validation_samples <- 10000

# considering 10000 words in dataset
max_words <- 10000

# tokenizing data
tokenizer <- text_tokenizer(num_words = max_words) %>%fit_text_tokenizer(texts)

# Turns strings into lists of integer indices
sequences <- texts_to_sequences(tokenizer, texts)

# recover the word index that was computed
word_index = tokenizer$word_index

```

```{r}
data <- pad_sequences(sequences, maxlen = maxlen)
labels <- as.array(labels)
indices <- sample(1:nrow(data))
training_indices <- indices[1:training_samples]

validation_indices <- indices[(training_samples + 1):(training_samples + validation_samples)]


x_train <- data[training_indices,]
y_train <- labels[training_indices]
x_val <- data[validation_indices,]
y_val <- labels[validation_indices]

# Tokenizing the data of the test set

test_dir <- file.path(imdb_dir, "test")
labels <- c()
texts <- c()
for (label_type in c("neg", "pos")) {
  label <- switch(label_type, neg = 0, pos = 1)
  dir_name <- file.path(test_dir, label_type)
  for (fname in list.files(dir_name, pattern = glob2rx("*.txt"),
                           full.names = TRUE)) {
    texts <- c(texts, readChar(fname, file.info(fname)$size))
    labels <- c(labels, label)
  }
}
sequences <- texts_to_sequences(tokenizer, texts)
x_test <- pad_sequences(sequences, maxlen = maxlen)
y_test <- as.array(labels)


#Training the  model 

model_w <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words, output_dim = 8, input_length = maxlen) %>%
  layer_flatten() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

model_w %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history_w <- model_w %>% fit(
  x_train, y_train,
  epochs = 10,
  batch_size = 32,
  validation_data = list(x_val, y_val)
)

plot(history_w)

# checking with epochs as 2 to view on test dataset 
 model_w %>% fit(
  x_train, y_train,
  epochs = 2 ,
  batch_size = 32)

reuslt_w <- model_w %>% evaluate(x_test,y_test)

reuslt_w



```


```{r echo=FALSE}
cat("The Test accuracy of the model is ",reuslt_w$acc)
```


```{r}
# Parsing the GloVe word-embeddings file

glove_dir = "C:/STUDY/MSBA/ADVANCED MACHINE LEARNING/Project/2/glove.6B"
lines <- readLines(file.path(glove_dir, "glove.6B.100d.txt"))

embeddings_index <- new.env(hash = TRUE, parent = emptyenv())
for (i in 1:length(lines)) {
  line <- lines[[i]]
  values <- strsplit(line, " ")[[1]]
  word <- values[[1]]
  embeddings_index[[word]] <- as.double(values[-1])
}

```

```{r}

# Preparing the GloVe word-embeddings matrix
embedding_dim <- 100
embedding_matrix <- array(0, c(max_words, embedding_dim))
for (word in names(word_index)) {
  index <- word_index[[word]]
  if (index < max_words) {
    embedding_vector <- embeddings_index[[word]]
    if (!is.null(embedding_vector))
      embedding_matrix[index+1,] <- embedding_vector
  }
}

model_p <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words, output_dim = embedding_dim,input_length = maxlen) %>%
  layer_flatten() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

# Loading pretrained word embeddings into the embedding layer
get_layer(model_p, index = 1) %>%
  set_weights(list(embedding_matrix)) %>%
  freeze_weights()

model_p %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history_p <- model_p %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val, y_val)
)

plot(history_p)
```

model is overfitting with small training samples 

```{r}
model_p %>% fit(
  x_train, y_train,
  epochs = 3,
  batch_size = 32)

reuslt_p <- model_p %>% evaluate(x_test,y_test)

reuslt_p
```

```{r echo=FALSE}
cat("The Test accuracy of the model is ",reuslt_p$acc)
```

